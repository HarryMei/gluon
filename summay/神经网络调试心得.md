## 前言

从开始接触mxnet的gluon教程开始，我就一直跟着官方教程自己动手做，也参加了kaggle的两个竞赛项目，到目前为止对训练调试过的神经网络是有更直观的了解了，对这些网络的优化和调试也多多少少有了一些的经验，所以乘着还没有忘记，赶紧总结记录出来（到目前为止做到的都是监督学习内容，所以本文也是以监督学习为对象说明）。

## 使用神经网络训练的步骤

这些天看过不少的深度神经网络的设计，也使用mxnet的gluon或者是直接手动编写过一些相对简单的网络来训练一些标准的数据集，体会最深的一点就是不管多么复杂的网络，变化的部分只是网络模型的本身，其他的部分做下来基本上都是属于同一个套路。

实现一个使用神经网络训练数据的项目，一般会有如下步骤：

1. 整理、导入数据，这是准备工作
2. 定义网络结构，这是算法的核心
3. 训练，将数据输入网络中，循环迭代
4. 测试，使用训练好的模型测试实际的数据

以下会对上面四个步骤分别说明。

## 数据整理和导入

虽然当我们在编写代码时，这只是准备性质的工作，但是在处理实际的机器学习、深度学习的问题时，数据才是核心：不同类型的数据决定了你采用哪一种算法会更好，而在实际训练时，数据的质量直接关系到最终结果的好坏。

深度学习需要大量的数据，然而收集大量的数据是一件很困难的事情，而这也是以前制约深度学习发展的一个很重要的因素。所幸的是，在大数据的今天，数据的收集和存储已经方便很多，对我们普通学习者而言更有利的是，前人已经整理出了不少标准的数据库提供我们使用和研究，如我所用过的：MNIST（手写数字图像）、FishionMNIST（服装图像）、CIFAR-10图像数据集。

在数据整理和导入时，有以下几点要注意。

### 1.  图像数据的整理

对于图像数据在整理时，训练的数据为了方便导入，一般都是根据类别新建文件夹，各自存放各自的的类型。像mxnet就很提供的图像文件夹导入函数接口，可以很方便的导入图片数据和标签。

### 2. 一些预处理方法

对于非图像类的原始数据，可能有：无效数据Nan，数据属性取值范围相差过大等等问题。这是我们往往需要做预处理，这就属于数据处理方面的内容了，很多数据处理的方法可以使用。

+ 对于无效数据：我们可以自己差值补全（如插平均值），如果属性Nan值过多，直接删掉也可以这一列
+ 对于属性值范围差异大的问题：对数据做归一化(或者说是标准化)处理

在归一化处理时：对于连续数值型的数据归一化，通常选择减平均值除以标准差的方法——```(x-x.mean())/x.std())```；在处理非数值非连续型的数据时，直接使用one-hot coding进行编码将一个属性变为多个0、1的属性，这样做的原因是可以统一用欧氏距离来评价数据点位置的远近。

## 定义网络模型

目前，我所接触到的神经网络层类型有两种：全连接层或卷积模块，我们定义网络就是将这些层或者模块叠加组合起来。在最简单的单层全连接网络的时候，我有用python手动编写，这在有提供矩阵运算的编程语言中实现是比较简单：

+ 直接用矩阵运算的线性方程组表示网络
+ 编写使用微分法求近似偏导函数
+ 编写梯度下降（或随机梯度下降）函数
+ 根据问题定义代价函数

以上，虽然只有第一点是确定网络本身结构，但是我还是觉得把这些核心算法放在一起比较好。

单层全连接网络可以简单的手动实现，主要还是网络结构简单，求偏导容易。复杂的网络结构就不方便手动编写了，最好是借助各种深度学习框架，这些框架都有自动求导功能，这些基于计算图的求导更准确，且借助BP算法能够计算更深层次网络节点的偏导。另外一点就是负责的模型，定义网络本身就很麻烦，如深层的卷积神经网络，每一层的权重参数、卷积核的大小都需要自己指定且相互之间还有印象，输入的图片经过这一层一层的网络，形状不断的在改变，而这些网络的参数本身就很让人混淆，再经过一系列变换，就更难确定了。

## 训练与测试


